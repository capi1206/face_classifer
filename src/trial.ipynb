{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f131b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a950883",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, # Desired output size for the cropped face\n",
    "    margin=0, \n",
    "    min_face_size=20, \n",
    "    thresholds=[0.6, 0.7, 0.7], # Detection thresholds for stages of MTCNN\n",
    "    factor=0.709, \n",
    "    post_process=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "def detect_and_crop(image_file_path, quality_threshold=0.95):\n",
    "    img = Image.open(image_file_path).convert('RGB')\n",
    "    \n",
    "    # boxes: bounding boxes, probs: confidence scores\n",
    "    boxes, probs = mtcnn.detect(img)\n",
    "    \n",
    "    # Use the prob output as your quality score\n",
    "    if boxes is None:\n",
    "        return [] # No face found\n",
    "    \n",
    "    good_quality_faces = []\n",
    "    for i, (box, prob) in enumerate(zip(boxes, probs)):\n",
    "        # Check against your desired quality/confidence threshold\n",
    "        if prob > quality_threshold:\n",
    "            box_np = np.array(box)\n",
    "            \n",
    "            # 2. Reshape the 1D box (4,) into a 2D array of shape (1, 4) \n",
    "            #    which is the expected batch format for coordinates.\n",
    "            box_array_2d = box_np.reshape(1, 4)\n",
    "            \n",
    "            try:\n",
    "                # Pass the 2D NumPy array directly.\n",
    "                face_tensor = mtcnn.extract(img, box_array_2d, save_path=None)\n",
    "                \n",
    "                if face_tensor is not None:\n",
    "                    good_quality_faces.append({\n",
    "                        'face_tensor': face_tensor, \n",
    "                        'confidence_score': prob\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting face for box {i} (score: {prob:.4f}): {e}\")\n",
    "    return good_quality_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af42237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zero(number):\n",
    "    if number < 10:\n",
    "        return \"00\" + str(number)\n",
    "    if number < 100:\n",
    "        return \"0\" +  str(number)\n",
    "    return str(number)\n",
    "\n",
    "faces = []\n",
    "for i in range(1, 702):\n",
    "    img_path = f'/Users/carlos/Documents/BlyzAI/face_classifer/data/archive/natural_images/dog/dog_0{add_zero(i)}.jpg'\n",
    "    ans = detect_and_crop(img_path, 0.6)\n",
    "    if len(ans) > 0:\n",
    "        best_one = sorted(ans, key=lambda x: x['confidence_score'], reverse=True)[0] \n",
    "        faces.append((best_one, img_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af9e0734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for face, img_path in faces:\n",
    "#     if face['confidence_score'] > 0.95:\n",
    "#         print(f\"path {img_path}, confidence score: {face['confidence_score']}\")\n",
    "len([f for f, _ in faces if f['confidence_score'] > 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd39797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "# Assuming mtcnn is initialized and imported\n",
    "\n",
    "def detect_and_crop_and_visualize(image_file_path, quality_threshold=0.95):\n",
    "    # Open the image and convert it to RGB\n",
    "    img = Image.open(image_file_path).convert('RGB')\n",
    "    \n",
    "    # Create a copy of the image to draw on (we don't want to modify the original img object used for mtcnn.detect)\n",
    "    visual_img = img.copy()\n",
    "    draw = ImageDraw.Draw(visual_img)\n",
    "\n",
    "    # boxes: bounding boxes (NumPy array), probs: confidence scores (NumPy array)\n",
    "    boxes, probs = mtcnn.detect(img)\n",
    "    \n",
    "    if boxes is None:\n",
    "        print(\"No face detected in the image.\")\n",
    "        return [], visual_img # Return the original image if no face is found\n",
    "    \n",
    "    good_quality_faces = []\n",
    "    \n",
    "    for i, (box, prob) in enumerate(zip(boxes, probs)):\n",
    "        # Check against your desired quality/confidence threshold\n",
    "        if prob > quality_threshold:\n",
    "            \n",
    "            # --- Draw the Bounding Box on the Visual Image ---\n",
    "            # Box coordinates are [x_min, y_min, x_max, y_max]\n",
    "            x_min, y_min, x_max, y_max = [int(b) for b in box]\n",
    "\n",
    "            # Draw a red rectangle (line width 2) for accepted faces\n",
    "            draw.rectangle([x_min, y_min, x_max, y_max], outline=(255, 0, 0), width=3)\n",
    "            \n",
    "            # Optionally add the confidence score as text\n",
    "            text = f\"{prob:.2f}\"\n",
    "            draw.text((x_min, y_min - 15), text, fill=(255, 0, 0))\n",
    "\n",
    "\n",
    "            # --- Face Extraction (Original Code) ---\n",
    "            box_np = np.array(box)\n",
    "            box_array_2d = box_np.reshape(1, 4)\n",
    "            \n",
    "            try:\n",
    "                face_tensor = mtcnn.extract(img, box_array_2d, save_path=None)\n",
    "                \n",
    "                if face_tensor is not None:\n",
    "                    good_quality_faces.append({\n",
    "                        'face_tensor': face_tensor, \n",
    "                        'confidence_score': prob,\n",
    "                        'bounding_box': box.tolist() # Store the coordinates\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting face for box {i} (score: {prob:.4f}): {e}\")\n",
    "\n",
    "    # Return the list of good faces AND the image with boxes drawn on it\n",
    "    return good_quality_faces, visual_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "888c0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your variables are defined and libraries are imported\n",
    "for face, img_path in faces:\n",
    "    if face['confidence_score'] > 0.96:\n",
    "\n",
    "         best_faces, image_with_boxes = detect_and_crop_and_visualize(img_path)\n",
    "\n",
    "         image_with_boxes.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3fdc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "##EMBEDDING EXTRACTION - STAGE 2##\n",
    "##\n",
    "# Initialize InceptionResnetV1 for embedding extraction\n",
    "resnet = InceptionResnetV1(\n",
    "    pretrained='vggface2' # Pre-trained weights on a large dataset\n",
    ").eval().to(device)\n",
    "\n",
    "def get_face_embedding(face_tensor):\n",
    "    # Pass the face tensor (from Stage 1) through the ResNet encoder\n",
    "    with torch.no_grad():\n",
    "        # The .unsqueeze(0) adds the batch dimension\n",
    "        embedding = resnet(face_tensor.to(device).unsqueeze(0))\n",
    "    # The output is a 512-dimensional vector (for InceptionResnetV1)\n",
    "    return embedding.squeeze(0).cpu().numpy() # Return as a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='casia-webface').eval()\n",
    "\n",
    "# Load two face images to be verified\n",
    "img1 = Image.open('path_to_image1.jpg')\n",
    "img2 = Image.open('path_to_image2.jpg')\n",
    "\n",
    "# Detect faces and extract embeddings\n",
    "faces1, _ = mtcnn.detect(img1)\n",
    "faces2, _ = mtcnn.detect(img2)\n",
    "\n",
    "if faces1 is not None and faces2 is not None:\n",
    "    aligned1 = mtcnn(img1)\n",
    "    aligned2 = mtcnn(img2)\n",
    "    embeddings1 = resnet(aligned1).detach()\n",
    "    embeddings2 = resnet(aligned2).detach()\n",
    "    \n",
    "    # Calculate the Euclidean distance between embeddings\n",
    "    distance = (embeddings1 - embeddings2).norm().item()\n",
    "    if distance < 1.0:  # You can adjust the threshold for verification\n",
    "        print(\"Same person\")\n",
    "    else:\n",
    "        print(\"Different persons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ClASSIFICATION - STAGE 3##\n",
    "##\n",
    "\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "\n",
    "# Assume this is loaded from your Enrollment/Data Service\n",
    "# Structure: {'Individual Name': numpy_array_centroid_embedding}\n",
    "KNOWN_FACE_CENTROIDS = {} \n",
    "\n",
    "def classify_identity(input_embedding, known_centroids, tolerance_threshold=0.9):\n",
    "    min_distance = float('inf')\n",
    "    best_match_id = \"Unknown\"\n",
    "    \n",
    "    for identity, centroid in known_centroids.items():\n",
    "        # Use Euclidean distance: smaller distance means higher similarity\n",
    "        distance = euclidean(input_embedding, centroid)\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            best_match_id = identity\n",
    "            \n",
    "    # Apply the tolerance threshold\n",
    "    # Note: Threshold selection is critical and requires training/validation\n",
    "    if min_distance <= tolerance_threshold:\n",
    "        return best_match_id, min_distance\n",
    "    else:\n",
    "        return \"Unknown\", min_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49f817a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def add_zero(number):\n",
    "    if number < 10:\n",
    "        return \"00\" + str(number)\n",
    "    if number < 100:\n",
    "        return \"0\" +  str(number)\n",
    "    return str(number)\n",
    "# Initialize MTCNN for face detection\n",
    "#Load two face images to be verified\n",
    "def get_embedding(img, mtcnn_model, resnet_model, device):\n",
    "    # 1. MTCNN Detection and Alignment\n",
    "    aligned = mtcnn_model(img) \n",
    "    \n",
    "    # 2. Handle Case 1: No Face Found (aligned is None)\n",
    "    if aligned is None:\n",
    "        print(\"Warning: No face detected in image.\")\n",
    "        # Return a zero vector or handle as an error\n",
    "        return None\n",
    "    \n",
    "    # 3. Handle Case 2: Single Face Found (aligned is (3, 160, 160))\n",
    "    # Check if the output is a single image (not a batch) and add the batch dimension\n",
    "    if aligned.dim() == 3:\n",
    "        aligned = aligned.unsqueeze(0)\n",
    "    \n",
    "    # Ensure the tensor is on the correct device (it should be if mtcnn was)\n",
    "    aligned = aligned.to(device)\n",
    "\n",
    "    # 4. ResNet Embedding Extraction\n",
    "    with torch.no_grad():\n",
    "        embeddings = resnet_model(aligned).detach()\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Load pre-trained Inception ResNet model\n",
    "resnet = InceptionResnetV1(pretrained='casia-webface').eval()\n",
    "\n",
    "def get_difference(img1, img2, mtcnn, resnet, device):\n",
    "    embeddings1 = get_embedding(img1, mtcnn, resnet, device)\n",
    "    embeddings2 = get_embedding(img2, mtcnn, resnet, device)\n",
    "    if embeddings1 is not None and embeddings2 is not None:\n",
    "        distance = (embeddings1 - embeddings2).norm().item()\n",
    "        return distance\n",
    "    else:\n",
    "        print(\"Cannot calculate distance due to missing face detection.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "PREPATH = '/Users/carlos/Documents/BlyzAI/face_classifer/data/archive/data/natural_images/person/person_0'\n",
    "\n",
    "for i in []: #range(100):\n",
    "    int1, int2 = random.randint(1, 985), random.randint(1, 986)\n",
    "    #print(f\"Comparing images {int1} and {int2}...\")\n",
    "    path1 = PREPATH + add_zero(int1) + '.jpg'\n",
    "    path2 = PREPATH + add_zero(int2) + '.jpg'\n",
    "    img1 = Image.open(path1).convert('RGB')\n",
    "    img2 = Image.open(path2).convert('RGB')\n",
    "    distance = get_difference(img1, img2, mtcnn, resnet, device)\n",
    "    if distance is not None and distance < 0.9:\n",
    "        print(f\"Images {int1} and {int2} are of the SAME person with distance {distance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d123da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# --- ASSUMING THESE ARE INITIALIZED ---\n",
    "# from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "# mtcnn = MTCNN(image_size=160, margin=0, device=device)\n",
    "# resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_single_face_embedding(frame_rgb, mtcnn_model, resnet_model, quality_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Detects faces in a single frame, extracts the embedding for the *highest confidence* face\n",
    "    that passes the quality threshold, and returns it as a NumPy array.\n",
    "    \"\"\"\n",
    "    boxes, probs = mtcnn_model.detect(frame_rgb)\n",
    "\n",
    "    if boxes is None:\n",
    "        return None # No face found\n",
    "\n",
    "    # 2. Select the Best Face\n",
    "    good_detections = [(box, prob) for box, prob in zip(boxes, probs) if prob > quality_threshold]\n",
    "    \n",
    "    if not good_detections:\n",
    "        return None # No face meets quality threshold\n",
    "    \n",
    "    # Select the box with the highest confidence score (best face)\n",
    "    best_box, best_prob = max(good_detections, key=lambda x: x[1])\n",
    "\n",
    "    # 3. Alignment, Cropping, and Normalization\n",
    "    try:\n",
    "        # Convert the best box from NumPy array (4,) to 2D NumPy array (1, 4)\n",
    "        box_array_2d = np.array(best_box).reshape(1, 4)\n",
    "        \n",
    "        # mtcnn.extract handles alignment and conversion to a PyTorch Tensor\n",
    "        face_tensor = mtcnn_model.extract(frame_rgb, box_array_2d, save_path=None)\n",
    "        \n",
    "        if face_tensor is None:\n",
    "            return None\n",
    "\n",
    "        # 4. Embedding Extraction (Encoder Stage)\n",
    "        with torch.no_grad():\n",
    "            # unsqueeze(0) adds the batch dimension (1, 3, 160, 160)\n",
    "            embedding = resnet_model(face_tensor.to(device).unsqueeze(0))\n",
    "        \n",
    "        return embedding.squeeze(0).cpu().numpy() # Return (512,) NumPy array\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Extraction error: {e}\") \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1632bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import random\n",
    "def process_video_for_embedding(video_path, frames_to_sample, mtcnn, resnet, quality_threshold=0.95):\n",
    "    \"\"\"Returns all face embeddings for a given video and specified frames\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video metadata\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if total_frames < frames_to_sample:\n",
    "        print(f\"Video is too short ({total_frames} frames). Sampling all frames.\")\n",
    "        frames_to_sample = total_frames\n",
    "\n",
    "    # Determine the step size for uniform sampling\n",
    "    if frames_to_sample == 0:\n",
    "        return None\n",
    "        \n",
    "    sampling_interval = max(1, total_frames // frames_to_sample)\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    # --- Main Sampling and Processing Loop ---\n",
    "    for i in range(total_frames):\n",
    "        # Set the current frame position\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        \n",
    "        # Read the frame\n",
    "        ret, frame = cap.read() \n",
    "        \n",
    "        # Check if frame read was successful and if it's a sample point\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if i % sampling_interval == 0:\n",
    "            # OpenCV reads in BGR format, convert to RGB PIL image for facenet-pytorch\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_pil = Image.fromarray(frame_rgb)\n",
    "            \n",
    "            # Get embedding for the best face in this frame\n",
    "            embedding = get_single_face_embedding(frame_pil, mtcnn, resnet, quality_threshold)\n",
    "            \n",
    "            if embedding is not None:\n",
    "                all_embeddings.append(embedding)\n",
    "\n",
    "    # Clean up\n",
    "    cap.release()\n",
    "    \n",
    "    # --- Embedding Aggregation (Averaging) ---\n",
    "    if not all_embeddings:\n",
    "        print(\"No high-quality faces were detected in the sampled frames.\")\n",
    "        return None\n",
    "        \n",
    "    # Stack the embeddings to form a matrix (N, 512)\n",
    "    embedding_matrix = np.stack(all_embeddings)\n",
    "    \n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d38878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "video_path = '/Users/carlos/Documents/BlyzAI/face_classifer/data/videos-dataset/files/1/3.mp4'\n",
    "#mtcnn = MTCNN()\n",
    "\n",
    "# Load pre-trained Inception ResNet model\n",
    "resnet = InceptionResnetV1(pretrained='casia-webface').eval()\n",
    "\n",
    "embeddings_video_3 = process_video_for_embedding(\n",
    "    video_path,\n",
    "    30,\n",
    "    mtcnn,\n",
    "    resnet    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a1514ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Min distance is [1.1759666]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "image_file_path = '/Users/carlos/Documents/BlyzAI/face_classifer/data/videos-dataset/files/9/2.jpg'\n",
    "img2 = Image.open(image_file_path).convert('RGB')\n",
    "embedding2 = get_embedding(img2, mtcnn, resnet, device)\n",
    "min_d = 2.2\n",
    "if embedding2 is not None:\n",
    "    for embedding in embeddings_video_3:\n",
    "        \n",
    "\n",
    "    # Convert the list of video embeddings into a single PyTorch tensor (N, 512)\n",
    "        video_tensor = torch.from_numpy(embedding).float().to(device)\n",
    "    \n",
    "\n",
    "        distance = (video_tensor - embedding2).norm(dim=1).cpu().numpy()\n",
    "        if distance < min_d:\n",
    "            min_d = distance\n",
    "            \n",
    "\n",
    "print(f\" Min distance is {min_d}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5074039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a059d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Downloading opencv-python-headless-4.12.0.88.tar.gz (95.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy<2.3.0,>=2 (from opencv-python-headless)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-macosx_10_9_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-macosx_10_9_x86_64.whl (21.2 MB)\n",
      "Building wheels for collected packages: opencv-python-headless\n",
      "  Building wheel for opencv-python-headless (pyproject.toml) ... \u001b[?25l/^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9febbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facenet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
